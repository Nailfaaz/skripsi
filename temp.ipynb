{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project structure created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define directory structure\n",
    "dirs = [\n",
    "    'configs',\n",
    "    'datasets/shenzhen/images/train',\n",
    "    'datasets/shenzhen/images/val',\n",
    "    'datasets/shenzhen/images/test',\n",
    "    'datasets/shenzhen/masks/train',\n",
    "    'datasets/shenzhen/masks/val',\n",
    "    'datasets/shenzhen/masks/test',\n",
    "    'data',\n",
    "    'models',\n",
    "    'engines',\n",
    "    'scripts',\n",
    "    'results/logs',\n",
    "    'results/checkpoints'\n",
    "]\n",
    "\n",
    "# Create directories\n",
    "for d in dirs:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define files to create\n",
    "files = [\n",
    "    'configs/default.yaml',\n",
    "    'configs/unet.yaml',\n",
    "    'data/dataset.py',\n",
    "    'models/__init__.py',\n",
    "    'engines/trainer.py',\n",
    "    'engines/evaluator.py',\n",
    "    'scripts/train.py',\n",
    "    'scripts/evaluate.py',\n",
    "    'scripts/predict.py',\n",
    "    'README.md'\n",
    "]\n",
    "\n",
    "# Create empty files\n",
    "for f in files:\n",
    "    Path(f).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(f).touch()\n",
    "\n",
    "print(\"Project structure created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\\raw\n",
      "datasets\\raw\\montgomery\n",
      "datasets\\raw\\shenzhen\n",
      "datasets\\raw\\montgomery\\images\n",
      "datasets\\raw\\montgomery\\masks\n",
      "datasets\\raw\\shenzhen\\images\n",
      "datasets\\raw\\shenzhen\\masks\n"
     ]
    }
   ],
   "source": [
    "# print structure of of a folder 'datasets' ignoring the png \n",
    "def print_structure(path, ignore_ext='.png'):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in dirs:\n",
    "            print(os.path.join(root, name))\n",
    "        for name in files:\n",
    "            if not name.endswith(ignore_ext):\n",
    "                print(os.path.join(root, name))\n",
    "\n",
    "# Print structure of the 'datasets' folder\n",
    "print_structure('datasets', ignore_ext='.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask datasets/processed/montgomery/masks/MCUCXR_0001_0.png contains both 0 and 1 values.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# check whether C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\processed\\montgomery\\masks\\MCUCXR_0001_0.png is 1 and 0\n",
    "def check_mask_value(mask_path):\n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"Mask file {mask_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    with open(mask_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "        if b'\\x00' in content and b'\\x01' in content:\n",
    "            print(f\"Mask {mask_path} contains both 0 and 1 values.\")\n",
    "        elif b'\\x00' in content:\n",
    "            print(f\"Mask {mask_path} contains only 0 values.\")\n",
    "        elif b'\\x01' in content:\n",
    "            print(f\"Mask {mask_path} contains only 1 values.\")\n",
    "        else:\n",
    "            print(f\"Mask {mask_path} contains neither 0 nor 1 values.\")\n",
    "\n",
    "# Check a specific mask file\n",
    "mask_file = 'datasets/processed/montgomery/masks/MCUCXR_0001_0.png'\n",
    "check_mask_value(mask_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split\\shenzhen\\test\\images\n",
      "Total .npy files in 'test/images': 86\n",
      "Copying 17 files to 'C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split-small\\shenzhen\\test\\images'\n",
      "------------------------------\n",
      "Processing: C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split\\shenzhen\\test\\masks\n",
      "Total .npy files in 'test/masks': 86\n",
      "Copying 17 files to 'C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split-small\\shenzhen\\test\\masks'\n",
      "------------------------------\n",
      "Processing: C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split\\shenzhen\\train\\images\n",
      "Total .npy files in 'train/images': 396\n",
      "Copying 79 files to 'C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split-small\\shenzhen\\train\\images'\n",
      "------------------------------\n",
      "Processing: C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split\\shenzhen\\train\\masks\n",
      "Total .npy files in 'train/masks': 396\n",
      "Copying 79 files to 'C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split-small\\shenzhen\\train\\masks'\n",
      "------------------------------\n",
      "Processing: C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split\\shenzhen\\val\\images\n",
      "Total .npy files in 'val/images': 84\n",
      "Copying 16 files to 'C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split-small\\shenzhen\\val\\images'\n",
      "------------------------------\n",
      "Processing: C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split\\shenzhen\\val\\masks\n",
      "Total .npy files in 'val/masks': 84\n",
      "Copying 16 files to 'C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split-small\\shenzhen\\val\\masks'\n",
      "------------------------------\n",
      "Small dataset creation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def create_small_dataset_with_subfolders(source_base_path, dest_base_path, percentage=0.20):\n",
    "    \"\"\"\n",
    "    Copies a percentage of files from 'images' and 'masks' subdirectories\n",
    "    within test/train/val folders to new destination directories,\n",
    "    maintaining the directory structure.\n",
    "\n",
    "    Args:\n",
    "        source_base_path (str): The base path of the original datasets (e.g., 'C:/Users/nailf/Desktop/skripsi-2/datasets/split/shenzhen').\n",
    "        dest_base_path (str): The base path for the new small datasets (e.g., 'C:/Users/nailf/Desktop/skripsi-2/datasets/split-small/shenzhen').\n",
    "        percentage (float): The percentage of files to copy (e.g., 0.20 for 20%).\n",
    "    \"\"\"\n",
    "    top_level_dirs = [\"test\", \"train\", \"val\"]\n",
    "    data_sub_dirs = [\"images\", \"masks\"] # The new level of subdirectories\n",
    "\n",
    "    for top_dir in top_level_dirs:\n",
    "        for data_sub_dir in data_sub_dirs:\n",
    "            source_dir = os.path.join(source_base_path, top_dir, data_sub_dir)\n",
    "            dest_dir = os.path.join(dest_base_path, top_dir, data_sub_dir)\n",
    "\n",
    "            print(f\"Processing: {source_dir}\")\n",
    "\n",
    "            # Create destination directory if it doesn't exist\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "            # List all files in the source directory (assuming .npy files are directly here)\n",
    "            # Filter for .npy files if that's the only type you're interested in\n",
    "            all_files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f)) and f.lower().endswith('.npy')]\n",
    "\n",
    "            if not all_files:\n",
    "                print(f\"No .npy files found in {source_dir}. Skipping.\")\n",
    "                print(\"-\" * 30)\n",
    "                continue\n",
    "\n",
    "            # Calculate the number of files to copy\n",
    "            num_files_to_copy = max(1, int(len(all_files) * percentage)) # Ensure at least 1 file if available\n",
    "\n",
    "            # Randomly select files\n",
    "            selected_files = random.sample(all_files, num_files_to_copy)\n",
    "\n",
    "            print(f\"Total .npy files in '{top_dir}/{data_sub_dir}': {len(all_files)}\")\n",
    "            print(f\"Copying {len(selected_files)} files to '{dest_dir}'\")\n",
    "\n",
    "            # Copy selected files to the destination\n",
    "            for file_name in selected_files:\n",
    "                source_file_path = os.path.join(source_dir, file_name)\n",
    "                dest_file_path = os.path.join(dest_dir, file_name)\n",
    "                try:\n",
    "                    shutil.copy2(source_file_path, dest_file_path) # copy2 preserves metadata\n",
    "                except Exception as e:\n",
    "                    print(f\"Error copying {source_file_path} to {dest_file_path}: {e}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "# Define your paths\n",
    "source_base_path = r\"C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split\\shenzhen\"\n",
    "dest_base_path = r\"C:\\Users\\nailf\\Desktop\\skripsi-2\\datasets\\split-small\\shenzhen\"\n",
    "\n",
    "# Run the function\n",
    "create_small_dataset_with_subfolders(source_base_path, dest_base_path, percentage=0.20)\n",
    "\n",
    "print(\"Small dataset creation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging backbone output...\n",
      "Backbone features shape: torch.Size([2, 7, 7, 768])\n",
      "Backbone features type: <class 'torch.Tensor'>\n",
      "Input shape: torch.Size([2, 3, 224, 224])\n",
      "Output shape: torch.Size([2, 2, 224, 224])\n",
      "Model parameters: 30,874,112\n"
     ]
    }
   ],
   "source": [
    "# path: models/swin_unet.py\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "\n",
    "class SwinUNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_channels: int = 3,\n",
    "                 n_classes: int = 2,\n",
    "                 img_size: int = 224,  # Changed to 224 (more standard)\n",
    "                 patch_size: int = 4,\n",
    "                 embed_dim: int = 96,  # Base embedding dimension\n",
    "                 depths = [2, 2, 6, 2],\n",
    "                 num_heads = [3, 6, 12, 24],\n",
    "                 window_size: int = 7,\n",
    "                 drop_rate: float = 0.0,\n",
    "                 drop_path_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Calculate number of patches and final feature map size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Build the Swin Transformer backbone\n",
    "        self.backbone = SwinTransformer(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=n_channels,\n",
    "            num_classes=0,  # No classification head\n",
    "            embed_dim=embed_dim,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads,\n",
    "            window_size=window_size,\n",
    "            mlp_ratio=4.0,\n",
    "            qkv_bias=True,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            ape=False,  # Absolute position embedding\n",
    "            patch_norm=True,\n",
    "            use_checkpoint=False\n",
    "        )\n",
    "        \n",
    "        # Get the number of features from the backbone\n",
    "        # Based on the debug output, the final feature dimension is 768\n",
    "        self.num_features = 768  # This matches the actual output from backbone\n",
    "        \n",
    "        # Calculate the spatial dimensions after all downsampling\n",
    "        # Based on debug output: 7x7 feature maps for 224x224 input\n",
    "        self.feature_size = 7  # This matches the actual spatial size\n",
    "        \n",
    "        # Decoder layers for upsampling\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        \n",
    "        # Current channels starts from the deepest features (768 from debug output)\n",
    "        current_channels = 768\n",
    "        \n",
    "        # Build decoder layers for progressive upsampling\n",
    "        # We need to go from 7x7 -> 14x14 -> 28x28 -> 56x56 -> 224x224\n",
    "        decoder_channels = [384, 192, 96, 48]  # Progressive channel reduction\n",
    "        \n",
    "        for target_channels in decoder_channels:\n",
    "            self.decoder_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        current_channels, \n",
    "                        target_channels, \n",
    "                        kernel_size=2, \n",
    "                        stride=2\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(target_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(\n",
    "                        target_channels, \n",
    "                        target_channels, \n",
    "                        kernel_size=3, \n",
    "                        padding=1\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(target_channels),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "            current_channels = target_channels\n",
    "        \n",
    "        # Final upsampling to original resolution  \n",
    "        # From current_channels (48) to final output\n",
    "        # We need to go from 56x56 -> 224x224 (4x upsampling)\n",
    "        self.final_upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                current_channels,  # 48 \n",
    "                24, \n",
    "                kernel_size=4, \n",
    "                stride=4\n",
    "            ),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(24, 12, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final classification head\n",
    "        self.head = nn.Conv2d(12, n_classes, kernel_size=1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Ensure input size is compatible\n",
    "        if H != self.img_size or W != self.img_size:\n",
    "            x = F.interpolate(x, size=(self.img_size, self.img_size), \n",
    "                            mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Get features from Swin Transformer backbone\n",
    "        features = self.backbone.forward_features(x)  # Returns [B, H, W, C] format\n",
    "        \n",
    "        # Handle the actual return format [B, H, W, C]\n",
    "        if len(features.shape) == 4:\n",
    "            # Convert from [B, H, W, C] to [B, C, H, W] for conv layers\n",
    "            features = features.permute(0, 3, 1, 2).contiguous()\n",
    "        elif len(features.shape) == 2:\n",
    "            # If it returns [B, C], we need to reshape to spatial format\n",
    "            h = w = self.feature_size\n",
    "            features = features.view(B, self.num_features, h, w)\n",
    "        elif len(features.shape) == 3:\n",
    "            # If it returns [B, L, C], reshape to spatial\n",
    "            B_feat, L, C_feat = features.shape\n",
    "            h = w = int(math.sqrt(L))\n",
    "            features = features.transpose(1, 2).contiguous().view(B, C_feat, h, w)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected feature shape: {features.shape}\")\n",
    "        \n",
    "        # Apply decoder layers\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            features = decoder_layer(features)\n",
    "        \n",
    "        # Final upsampling to original resolution\n",
    "        features = self.final_upsample(features)\n",
    "        \n",
    "        # Apply classification head\n",
    "        output = self.head(features)\n",
    "        \n",
    "        # Resize to match original input size if needed\n",
    "        if output.shape[-2:] != (H, W):\n",
    "            output = F.interpolate(output, size=(H, W), \n",
    "                                 mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the model\n",
    "    model = SwinUNet(\n",
    "        n_channels=3,\n",
    "        n_classes=2,\n",
    "        img_size=224,\n",
    "        patch_size=4,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24]\n",
    "    )\n",
    "    \n",
    "    # Test with dummy input\n",
    "    x = torch.randn(2, 3, 224, 224)\n",
    "    \n",
    "    # Debug: Check what the backbone returns\n",
    "    print(\"Debugging backbone output...\")\n",
    "    with torch.no_grad():\n",
    "        backbone_features = model.backbone.forward_features(x)\n",
    "        print(f\"Backbone features shape: {backbone_features.shape}\")\n",
    "        print(f\"Backbone features type: {type(backbone_features)}\")\n",
    "        \n",
    "        # Test full forward pass\n",
    "        try:\n",
    "            output = model(x)\n",
    "            print(f\"Input shape: {x.shape}\")\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "            print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass: {e}\")\n",
    "            print(\"This helps identify the exact issue with the backbone output format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
